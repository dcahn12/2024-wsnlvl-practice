{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets as da\n",
    "\n",
    "from optimizers import AdamOptimizer\n",
    "from optimizers.lr_schedulers import InverseSquareRootSchedule\n",
    "\n",
    "from models.transformer import DualTransformer\n",
    "from models.loss import ivc_loss, cal_nll_loss, rec_loss\n",
    "\n",
    "import collections\n",
    "from utils_helper import TimeMeter, AverageMeter\n",
    "from utils_helper import load_json, top_1_metric, top_n_metric, move_to_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './config/charades/main.json'\n",
    "tag = 'default_charades'\n",
    "seed = 8\n",
    "num_updates = 0 # For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2)\n",
    "torch.cuda.manual_seed(seed + 4)\n",
    "torch.cuda.manual_seed_all(seed + 4)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_json(config_path)\n",
    "args['train']['model_saved_path'] = os.path.join(args['train']['model_saved_path'], tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Dataset\n",
    "\n",
    "with open(args['dataset']['vocab_path'], 'rb') as fp:\n",
    "    vocab = pickle.load(fp)\n",
    "    \n",
    "cls = getattr(da, args['dataset']['dataset'], None)\n",
    "train_set = cls(data_path=args['dataset']['train_data'], vocab=vocab, args=args['dataset'], is_training=True, split='train')\n",
    "test_set = cls(data_path=args['dataset']['test_data'], vocab=vocab, args=args['dataset'], split='test')\n",
    "batch_size = args['train']['batch_size']\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed + 1)\n",
    "        torch.manual_seed(seed + 3)\n",
    "        torch.cuda.manual_seed(seed + 4)\n",
    "        torch.cuda.manual_seed_all(seed + 4)\n",
    "\n",
    "    set_seed(8 + worker_id)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=train_set.collate_data, num_workers=2, worker_init_fn=worker_init_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=test_set.collate_data, num_workers=0)\n",
    "\n",
    "args['model']['config']['vocab_size'] = train_set.vocab_size\n",
    "args['model']['config']['max_epoch'] = args['train']['max_num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model (CPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
    "\n",
    "    Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, padding_idx, init_size=1024):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "            init_size,\n",
    "            embedding_dim,\n",
    "            padding_idx,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
    "        \"\"\"Build sinusoidal embeddings.\n",
    "\n",
    "        This matches the implementation in tensor2tensor, but differs slightly\n",
    "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "        \"\"\"\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
    "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
    "        if embedding_dim % 2 == 1:\n",
    "            # zero pad\n",
    "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
    "        if padding_idx is not None:\n",
    "            emb[padding_idx, :] = 0\n",
    "        return emb\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        bsz, seq_len, _ = input.size()\n",
    "        max_pos = seq_len\n",
    "        if self.weights is None or max_pos > self.weights.size(0):\n",
    "            # recompute/expand embeddings if needed\n",
    "            self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "                max_pos,\n",
    "                self.embedding_dim,\n",
    "                self.padding_idx,\n",
    "            )\n",
    "        self.weights = self.weights.cuda(input.device)[:max_pos]\n",
    "        return self.weights.unsqueeze(0)\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum number of supported positions.\"\"\"\n",
    "        return int(1e5)  # an arbitrary large number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPL(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dropout = config['dropout']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.sigma = config[\"sigma\"]\n",
    "        self.use_negative = config['use_negative']\n",
    "        self.num_props = config['num_props']\n",
    "        self.max_epoch = config['max_epoch']\n",
    "        self.gamma = config['gamma']\n",
    "\n",
    "        self.frame_fc = nn.Linear(config['frames_input_size'], config['hidden_size'])\n",
    "        self.word_fc = nn.Linear(config['words_input_size'], config['hidden_size'])\n",
    "        self.mask_vec = nn.Parameter(torch.zeros(config['words_input_size']).float(), requires_grad=True)\n",
    "        self.start_vec = nn.Parameter(torch.zeros(config['words_input_size']).float(), requires_grad=True)\n",
    "        self.pred_vec = nn.Parameter(torch.zeros(config['frames_input_size']).float(), requires_grad=True)\n",
    "\n",
    "        self.trans = DualTransformer(**config['DualTransformer'])\n",
    "        self.fc_comp = nn.Linear(config['hidden_size'], self.vocab_size)\n",
    "        self.fc_gauss = nn.Linear(config['hidden_size'], self.num_props*2)\n",
    " \n",
    "        self.word_pos_encoder = SinusoidalPositionalEmbedding(config['hidden_size'], 0, 20)\n",
    "\n",
    "    def forward(self, frames_feat, frames_len, words_id, words_feat, words_len, weights, **kwargs):\n",
    "        bsz, n_frames, _ = frames_feat.shape\n",
    "        pred_vec = self.pred_vec.view(1, 1, -1).expand(bsz, 1, -1)\n",
    "        frames_feat = torch.cat([frames_feat, pred_vec], dim=1)\n",
    "        frames_feat = F.dropout(frames_feat, self.dropout, self.training)\n",
    "        frames_feat = self.frame_fc(frames_feat)\n",
    "        frames_mask = self._generate_mask(frames_feat, frames_len)\n",
    "\n",
    "        words_feat[:, 0] = self.start_vec.cuda()\n",
    "        words_pos = self.word_pos_encoder(words_feat)\n",
    "        words_feat = F.dropout(words_feat, self.dropout, self.training)\n",
    "        words_feat = self.word_fc(words_feat)\n",
    "        words_mask = self._generate_mask(words_feat, words_len + 1)\n",
    "\n",
    "        # generate Gaussian masks\n",
    "        enc_out, h = self.trans(frames_feat, frames_mask, words_feat + words_pos, words_mask, decoding=1)\n",
    "        gauss_param = torch.sigmoid(self.fc_gauss(h[:, -1])).view(bsz*self.num_props, 2)\n",
    "        gauss_center = gauss_param[:, 0]\n",
    "        gauss_width = gauss_param[:, 1]\n",
    "\n",
    "        # downsample for effeciency\n",
    "        props_len = n_frames//4\n",
    "        keep_idx = torch.linspace(0, n_frames-1, steps=props_len).long()\n",
    "        frames_feat = frames_feat[:, keep_idx]\n",
    "        frames_mask = frames_mask[:, keep_idx]\n",
    "        props_feat = frames_feat.unsqueeze(1).expand(bsz, self.num_props, -1, -1).contiguous().view(bsz*self.num_props, props_len, -1)\n",
    "        props_mask = frames_mask.unsqueeze(1).expand(bsz, self.num_props, -1).contiguous().view(bsz*self.num_props, -1)\n",
    "\n",
    "        gauss_weight = self.generate_gauss_weight(props_len, gauss_center, gauss_width)\n",
    "        \n",
    "        # semantic completion\n",
    "        words_feat, masked_words = self._mask_words(words_feat, words_len, weights=weights)\n",
    "        words_feat = words_feat + words_pos\n",
    "        words_feat = words_feat[:, :-1]\n",
    "        words_mask = words_mask[:, :-1]\n",
    "\n",
    "        words_mask1 = words_mask.unsqueeze(1).expand(bsz, self.num_props, -1).contiguous().view(bsz*self.num_props, -1)\n",
    "        words_id1 = words_id.unsqueeze(1).expand(bsz, self.num_props, -1).contiguous().view(bsz*self.num_props, -1)\n",
    "        words_feat1 = words_feat.unsqueeze(1).expand(bsz, self.num_props, -1, -1).contiguous().view(bsz*self.num_props, words_mask1.size(1), -1)\n",
    "\n",
    "        pos_weight = gauss_weight/gauss_weight.max(dim=-1, keepdim=True)[0]\n",
    "        _, h, attn_weight = self.trans(props_feat, props_mask, words_feat1, words_mask1, decoding=2, gauss_weight=pos_weight, need_weight=True)\n",
    "        words_logit = self.fc_comp(h)\n",
    "\n",
    "        if self.use_negative:\n",
    "            neg_1_weight, neg_2_weight = self.negative_proposal_mining(props_len, gauss_center, gauss_width, kwargs['epoch'])\n",
    "            \n",
    "            _, neg_h_1 = self.trans(props_feat, props_mask, words_feat1, words_mask1, decoding=2, gauss_weight=neg_1_weight)\n",
    "            neg_words_logit_1 = self.fc_comp(neg_h_1)\n",
    "  \n",
    "            _, neg_h_2 = self.trans(props_feat, props_mask, words_feat1, words_mask1, decoding=2, gauss_weight=neg_2_weight)\n",
    "            neg_words_logit_2 = self.fc_comp(neg_h_2)\n",
    "\n",
    "            _, ref_h = self.trans(frames_feat, frames_mask, words_feat, words_mask, decoding=2)\n",
    "            ref_words_logit = self.fc_comp(ref_h)\n",
    "        else:\n",
    "            neg_words_logit_1 = None\n",
    "            neg_words_logit_2 = None\n",
    "            ref_words_logit = None\n",
    "\n",
    "        return {\n",
    "            'neg_words_logit_1': neg_words_logit_1,\n",
    "            'neg_words_logit_2': neg_words_logit_2,\n",
    "            'ref_words_logit': ref_words_logit,\n",
    "            'words_logit': words_logit,\n",
    "            'words_id': words_id,\n",
    "            'words_mask': words_mask,\n",
    "            'width': gauss_width,\n",
    "            'center': gauss_center,\n",
    "            'gauss_weight': gauss_weight,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def generate_gauss_weight(self, props_len, center, width):\n",
    "        # pdb.set_trace()\n",
    "        weight = torch.linspace(0, 1, props_len)\n",
    "        weight = weight.view(1, -1).expand(center.size(0), -1).to(center.device)\n",
    "        center = center.unsqueeze(-1)\n",
    "        width = width.unsqueeze(-1).clamp(1e-2) / self.sigma\n",
    "\n",
    "        w = 0.3989422804014327\n",
    "        weight = w/width*torch.exp(-(weight-center)**2/(2*width**2))\n",
    "\n",
    "        return weight/weight.max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "\n",
    "    def negative_proposal_mining(self, props_len, center, width, epoch):\n",
    "        def Gauss(pos, w1, c):\n",
    "            w1 = w1.unsqueeze(-1).clamp(1e-2) / (self.sigma/2)\n",
    "            c = c.unsqueeze(-1)\n",
    "            w = 0.3989422804014327\n",
    "            y1 = w/w1*torch.exp(-(pos-c)**2/(2*w1**2))\n",
    "            return y1/y1.max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "        weight = torch.linspace(0, 1, props_len)\n",
    "        weight = weight.view(1, -1).expand(center.size(0), -1).to(center.device)\n",
    "\n",
    "        left_width = torch.clamp(center-width/2, min=0)\n",
    "        left_center = left_width * min(epoch/self.max_epoch, 1)**self.gamma * 0.5\n",
    "        right_width = torch.clamp(1-center-width/2, min=0)\n",
    "        right_center = 1 - right_width * min(epoch/self.max_epoch, 1)**self.gamma * 0.5\n",
    "\n",
    "        left_neg_weight = Gauss(weight, left_center, left_center)\n",
    "        right_neg_weight = Gauss(weight, 1-right_center, right_center)\n",
    "\n",
    "        return left_neg_weight, right_neg_weight\n",
    "\n",
    "    def _mask_words(self, words_feat, words_len, weights=None):\n",
    "        token = self.mask_vec.cuda().unsqueeze(0).unsqueeze(0)\n",
    "        token = self.word_fc(token)\n",
    "\n",
    "        masked_words = []\n",
    "        for i, l in enumerate(words_len):\n",
    "            l = int(l)\n",
    "            num_masked_words = max(l // 3, 1) \n",
    "            masked_words.append(torch.zeros([words_feat.size(1)]).byte().cuda())\n",
    "            if l < 1:\n",
    "                continue\n",
    "            p = weights[i, :l].cpu().numpy() if weights is not None else None\n",
    "            choices = np.random.choice(np.arange(1, l + 1), num_masked_words, replace=False, p=p)\n",
    "            masked_words[-1][choices] = 1\n",
    "        \n",
    "        masked_words = torch.stack(masked_words, 0).unsqueeze(-1)\n",
    "        masked_words_vec = words_feat.new_zeros(*words_feat.size()) + token\n",
    "        masked_words_vec = masked_words_vec.masked_fill_(masked_words == 0, 0)\n",
    "        words_feat1 = words_feat.masked_fill(masked_words == 1, 0) + masked_words_vec\n",
    "        return words_feat1, masked_words\n",
    "    \n",
    "    def _generate_mask(self, x, x_len):\n",
    "        if False and int(x_len.min()) == x.size(1):\n",
    "            mask = None\n",
    "        else:\n",
    "            mask = []\n",
    "            for l in x_len:\n",
    "                mask.append(torch.zeros([x.size(1)]).byte().cuda())\n",
    "                mask[-1][:l] = 1\n",
    "            mask = torch.stack(mask, 0)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5375680 Trainable parameters: 5375680\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "model_config = args['model']['config']\n",
    "model = CPL(model_config)\n",
    "model = model.cuda()\n",
    "\n",
    "total_num = sum(p.numel() for p in model.parameters())\n",
    "trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Total parameters:', total_num, 'Trainable parameters:', trainable_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer & Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer & Learning rate scheduler\n",
    "\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "args_optim = args['train'][\"optimizer\"]\n",
    "optimizer = AdamOptimizer(args_optim, parameters)\n",
    "lr_scheduler = InverseSquareRootSchedule(args['train']['optimizer'], optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_one_epoch(model, train_loader, epoch, optimizer, lr_scheduler, num_updates, args, **kwargs):\n",
    "    model.train()\n",
    "\n",
    "    def print_log():\n",
    "        msg = 'Epoch {}, Batch {}, lr = {:.5f}, '.format(epoch, bid, curr_lr)\n",
    "        for k, v in loss_meter.items():\n",
    "            msg += '{} = {:.4f}, '.format(k, v.avg)\n",
    "            v.reset()\n",
    "        msg += '{:.3f} seconds/batch'.format(1.0 / time_meter.avg)\n",
    "        print(msg)\n",
    "\n",
    "    display_n_batches, bid = 50, 0\n",
    "    time_meter = TimeMeter()\n",
    "    loss_meter = collections.defaultdict(lambda: AverageMeter())\n",
    "\n",
    "    for bid, batch in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        net_input = move_to_cuda(batch['net_input'])\n",
    "        output = model(epoch=epoch, **net_input)\n",
    "\n",
    "        loss, loss_dict = rec_loss(**output, num_props=model.num_props, **args['loss'])\n",
    "        rnk_loss, rnk_loss_dict = ivc_loss(**output, num_props=model.num_props, **args['loss'])\n",
    "        loss_dict.update(rnk_loss_dict)\n",
    "        loss = loss + rnk_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        num_updates += 1\n",
    "        curr_lr = lr_scheduler.step_update(num_updates)\n",
    "        time_meter.update()\n",
    "        for k, v in loss_dict.items():\n",
    "            loss_meter[k].update(v)\n",
    "\n",
    "        if bid % display_n_batches == 0:\n",
    "            print_log()\n",
    "\n",
    "    if bid % display_n_batches != 0:\n",
    "        print_log()\n",
    "        \n",
    "    return num_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cpl(model, test_loader, save=None, epoch=0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        metrics_logger = collections.defaultdict(lambda: AverageMeter())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for bid, batch in enumerate(test_loader, 1):\n",
    "                durations = np.asarray([i[1] for i in batch['raw']])\n",
    "                gt = np.asarray([i[2] for i in batch['raw']])\n",
    "\n",
    "                net_input = move_to_cuda(batch['net_input'])\n",
    "                output = model(epoch=epoch, **net_input)\n",
    "                bsz = len(durations)\n",
    "                num_props = model.num_props\n",
    "                k = min(num_props, 5)\n",
    "                \n",
    "                words_mask = output['words_mask'].unsqueeze(1).expand(bsz, num_props, -1).contiguous().view(bsz*num_props, -1)\n",
    "                words_id = output['words_id'].unsqueeze(1).expand(bsz, num_props, -1).contiguous().view(bsz*num_props, -1)\n",
    "\n",
    "                nll_loss, _ = cal_nll_loss(output['words_logit'], words_id, words_mask)\n",
    "                idx = nll_loss.view(bsz, num_props).argsort(dim=-1)\n",
    "\n",
    "                width = output['width'].view(bsz, num_props).gather(index=idx, dim=-1)\n",
    "                center = output['center'].view(bsz, num_props).gather(index=idx, dim=-1)\n",
    "                selected_props = torch.stack([torch.clamp(center-width/2, min=0), torch.clamp(center+width/2, max=1)], dim=-1)\n",
    "                selected_props = selected_props.cpu().numpy()\n",
    "                gt = gt / durations[:, np.newaxis]\n",
    "                \n",
    "                res = top_1_metric(selected_props[:, 0], gt)\n",
    "                \n",
    "                for key, v in res.items():\n",
    "                    metrics_logger['R@1,'+key].update(v, bsz)\n",
    "                res = top_n_metric(selected_props[:, :k].transpose(1, 0, 2), gt)\n",
    "                for key, v in res.items():\n",
    "                    metrics_logger['R@%d,'%(k)+key].update(v, bsz)\n",
    "\n",
    "        msg = '|'.join([' {} {:.4f} '.format(k, v.avg) for k, v in metrics_logger.items()])\n",
    "        print('|'+msg+'|')\n",
    "        return metrics_logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Epoch 1\n",
      "Epoch 1, Batch 50, lr = 0.00005, final_loss = 6.2458, nll_loss = 6.2001, ref_nll_loss = 6.2914, ivc_loss = 0.4884, neg_loss_1 = 0.0478, neg_loss_2 = 0.0487, ref_loss = 0.0390, div_loss = 0.0706, 0.212 seconds/batch\n",
      "Epoch 1, Batch 100, lr = 0.00010, final_loss = 4.7244, nll_loss = 4.6914, ref_nll_loss = 4.7575, ivc_loss = 0.2237, neg_loss_1 = 0.0452, neg_loss_2 = 0.0464, ref_loss = 0.0471, div_loss = 0.0170, 0.189 seconds/batch\n",
      "Epoch 1, Batch 150, lr = 0.00015, final_loss = 4.0648, nll_loss = 4.0231, ref_nll_loss = 4.1065, ivc_loss = 0.1804, neg_loss_1 = 0.0415, neg_loss_2 = 0.0395, ref_loss = 0.0390, div_loss = 0.0121, 0.180 seconds/batch\n",
      "Epoch 1, Batch 200, lr = 0.00020, final_loss = 3.6682, nll_loss = 3.6203, ref_nll_loss = 3.7162, ivc_loss = 0.1531, neg_loss_1 = 0.0367, neg_loss_2 = 0.0377, ref_loss = 0.0357, div_loss = 0.0086, 0.176 seconds/batch\n",
      "Epoch 1, Batch 250, lr = 0.00025, final_loss = 3.4320, nll_loss = 3.3764, ref_nll_loss = 3.4876, ivc_loss = 0.1423, neg_loss_1 = 0.0352, neg_loss_2 = 0.0344, ref_loss = 0.0305, div_loss = 0.0084, 0.175 seconds/batch\n",
      "Epoch 1, Batch 300, lr = 0.00030, final_loss = 3.3175, nll_loss = 3.2624, ref_nll_loss = 3.3726, ivc_loss = 0.1369, neg_loss_1 = 0.0369, neg_loss_2 = 0.0318, ref_loss = 0.0318, div_loss = 0.0073, 0.173 seconds/batch\n",
      "Epoch 1, Batch 332, lr = 0.00033, final_loss = 3.2344, nll_loss = 3.1737, ref_nll_loss = 3.2951, ivc_loss = 0.1166, neg_loss_1 = 0.0324, neg_loss_2 = 0.0274, ref_loss = 0.0280, div_loss = 0.0058, 0.172 seconds/batch\n",
      "save model to checkpoints/charades/default_charades/model-1.pt, num_updates 332.\n",
      "| R@1,mIoU 0.3266 | R@1,IoU@0.1 0.6222 | R@1,IoU@0.3 0.4946 | R@1,IoU@0.5 0.3407 | R@1,IoU@0.7 0.1425 | R@1,IoU@0.9 0.0247 | R@5,mIoU 0.6439 | R@5,IoU@0.1 0.9892 | R@5,IoU@0.3 0.9503 | R@5,IoU@0.5 0.7730 | R@5,IoU@0.7 0.4478 | R@5,IoU@0.9 0.0855 |\n",
      "Best results have been updated.\n",
      "============================================================\n",
      "Start Epoch 2\n",
      "Epoch 2, Batch 50, lr = 0.00038, final_loss = 3.1743, nll_loss = 3.1136, ref_nll_loss = 3.2351, ivc_loss = 0.1209, neg_loss_1 = 0.0311, neg_loss_2 = 0.0277, ref_loss = 0.0301, div_loss = 0.0064, 0.178 seconds/batch\n",
      "Epoch 2, Batch 100, lr = 0.00038, final_loss = 3.0956, nll_loss = 3.0318, ref_nll_loss = 3.1595, ivc_loss = 0.1169, neg_loss_1 = 0.0305, neg_loss_2 = 0.0250, ref_loss = 0.0313, div_loss = 0.0060, 0.173 seconds/batch\n",
      "Epoch 2, Batch 150, lr = 0.00036, final_loss = 3.0237, nll_loss = 2.9559, ref_nll_loss = 3.0915, ivc_loss = 0.1066, neg_loss_1 = 0.0247, neg_loss_2 = 0.0217, ref_loss = 0.0299, div_loss = 0.0061, 0.170 seconds/batch\n",
      "Epoch 2, Batch 200, lr = 0.00035, final_loss = 2.9763, nll_loss = 2.9030, ref_nll_loss = 3.0496, ivc_loss = 0.0944, neg_loss_1 = 0.0236, neg_loss_2 = 0.0185, ref_loss = 0.0292, div_loss = 0.0046, 0.169 seconds/batch\n",
      "Epoch 2, Batch 250, lr = 0.00033, final_loss = 2.8980, nll_loss = 2.8234, ref_nll_loss = 2.9727, ivc_loss = 0.1018, neg_loss_1 = 0.0215, neg_loss_2 = 0.0189, ref_loss = 0.0282, div_loss = 0.0066, 0.168 seconds/batch\n",
      "Epoch 2, Batch 300, lr = 0.00032, final_loss = 2.8481, nll_loss = 2.7712, ref_nll_loss = 2.9251, ivc_loss = 0.0852, neg_loss_1 = 0.0189, neg_loss_2 = 0.0180, ref_loss = 0.0264, div_loss = 0.0044, 0.167 seconds/batch\n",
      "Epoch 2, Batch 332, lr = 0.00031, final_loss = 2.7817, nll_loss = 2.7076, ref_nll_loss = 2.8557, ivc_loss = 0.0836, neg_loss_1 = 0.0192, neg_loss_2 = 0.0148, ref_loss = 0.0293, div_loss = 0.0041, 0.167 seconds/batch\n",
      "save model to checkpoints/charades/default_charades/model-2.pt, num_updates 664.\n",
      "| R@1,mIoU 0.3900 | R@1,IoU@0.1 0.6960 | R@1,IoU@0.3 0.5966 | R@1,IoU@0.5 0.4310 | R@1,IoU@0.7 0.1903 | R@1,IoU@0.9 0.0260 | R@5,mIoU 0.6703 | R@5,IoU@0.1 0.9883 | R@5,IoU@0.3 0.9687 | R@5,IoU@0.5 0.8309 | R@5,IoU@0.7 0.4943 | R@5,IoU@0.9 0.0811 |\n",
      "Best results have been updated.\n",
      "============================================================\n",
      "Start Epoch 3\n",
      "Epoch 3, Batch 50, lr = 0.00030, final_loss = 2.7613, nll_loss = 2.6830, ref_nll_loss = 2.8396, ivc_loss = 0.0842, neg_loss_1 = 0.0220, neg_loss_2 = 0.0145, ref_loss = 0.0275, div_loss = 0.0041, 0.187 seconds/batch\n",
      "Epoch 3, Batch 100, lr = 0.00029, final_loss = 2.7937, nll_loss = 2.7138, ref_nll_loss = 2.8735, ivc_loss = 0.0786, neg_loss_1 = 0.0197, neg_loss_2 = 0.0144, ref_loss = 0.0264, div_loss = 0.0036, 0.174 seconds/batch\n",
      "Epoch 3, Batch 150, lr = 0.00028, final_loss = 2.7500, nll_loss = 2.6722, ref_nll_loss = 2.8278, ivc_loss = 0.0828, neg_loss_1 = 0.0203, neg_loss_2 = 0.0149, ref_loss = 0.0297, div_loss = 0.0036, 0.173 seconds/batch\n",
      "Epoch 3, Batch 200, lr = 0.00027, final_loss = 2.7353, nll_loss = 2.6515, ref_nll_loss = 2.8192, ivc_loss = 0.0702, neg_loss_1 = 0.0143, neg_loss_2 = 0.0137, ref_loss = 0.0252, div_loss = 0.0034, 0.171 seconds/batch\n",
      "Epoch 3, Batch 250, lr = 0.00026, final_loss = 2.7368, nll_loss = 2.6515, ref_nll_loss = 2.8221, ivc_loss = 0.0707, neg_loss_1 = 0.0150, neg_loss_2 = 0.0128, ref_loss = 0.0266, div_loss = 0.0033, 0.170 seconds/batch\n",
      "Epoch 3, Batch 300, lr = 0.00026, final_loss = 2.7110, nll_loss = 2.6266, ref_nll_loss = 2.7955, ivc_loss = 0.0716, neg_loss_1 = 0.0165, neg_loss_2 = 0.0132, ref_loss = 0.0260, div_loss = 0.0032, 0.174 seconds/batch\n",
      "Epoch 3, Batch 332, lr = 0.00025, final_loss = 2.7191, nll_loss = 2.6363, ref_nll_loss = 2.8020, ivc_loss = 0.0728, neg_loss_1 = 0.0158, neg_loss_2 = 0.0148, ref_loss = 0.0252, div_loss = 0.0034, 0.173 seconds/batch\n",
      "save model to checkpoints/charades/default_charades/model-3.pt, num_updates 996.\n",
      "| R@1,mIoU 0.4067 | R@1,IoU@0.1 0.7188 | R@1,IoU@0.3 0.6200 | R@1,IoU@0.5 0.4535 | R@1,IoU@0.7 0.2096 | R@1,IoU@0.9 0.0275 | R@5,mIoU 0.6735 | R@5,IoU@0.1 0.9905 | R@5,IoU@0.3 0.9683 | R@5,IoU@0.5 0.8401 | R@5,IoU@0.7 0.5016 | R@5,IoU@0.9 0.0814 |\n",
      "Best results have been updated.\n",
      "============================================================\n",
      "Start Epoch 4\n",
      "Epoch 4, Batch 50, lr = 0.00025, final_loss = 2.6545, nll_loss = 2.5653, ref_nll_loss = 2.7438, ivc_loss = 0.0660, neg_loss_1 = 0.0152, neg_loss_2 = 0.0115, ref_loss = 0.0225, div_loss = 0.0033, 0.183 seconds/batch\n",
      "Epoch 4, Batch 100, lr = 0.00024, final_loss = 2.6580, nll_loss = 2.5702, ref_nll_loss = 2.7458, ivc_loss = 0.0700, neg_loss_1 = 0.0151, neg_loss_2 = 0.0131, ref_loss = 0.0263, div_loss = 0.0031, 0.174 seconds/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(model_saved_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o755\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_saved_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[0;32m----> 9\u001b[0m num_updates \u001b[38;5;241m=\u001b[39m \u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_updates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model save\u001b[39;00m\n\u001b[1;32m     12\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_updates\u001b[39m\u001b[38;5;124m'\u001b[39m: num_updates,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m: args,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     16\u001b[0m }\n",
      "Cell \u001b[0;32mIn[116], line 22\u001b[0m, in \u001b[0;36m_train_one_epoch\u001b[0;34m(model, train_loader, epoch, optimizer, lr_scheduler, num_updates, args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m model(epoch\u001b[38;5;241m=\u001b[39mepoch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnet_input)\n\u001b[1;32m     21\u001b[0m loss, loss_dict \u001b[38;5;241m=\u001b[39m rec_loss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput, num_props\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_props, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 22\u001b[0m rnk_loss, rnk_loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mivc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_props\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_props\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss_dict\u001b[38;5;241m.\u001b[39mupdate(rnk_loss_dict)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m rnk_loss\n",
      "File \u001b[0;32m/dataset/cpl/cpl/models/loss.py:102\u001b[0m, in \u001b[0;36mivc_loss\u001b[0;34m(words_logit, words_id, words_mask, num_props, neg_words_logit_1, neg_words_logit_2, ref_words_logit, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m div_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(target \u001b[38;5;241m-\u001b[39m source, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m div_loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, {\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mivc_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_loss_1\u001b[39m\u001b[38;5;124m'\u001b[39m: neg_loss_1\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m neg_words_logit_1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_loss_2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mneg_loss_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m neg_words_logit_2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: ref_loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m ref_words_logit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: div_loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    105\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train & Eval\n",
    "best_results = None\n",
    "for epoch in range(1, args['train']['max_num_epochs']+1):\n",
    "    print('Start Epoch {}'.format(epoch))\n",
    "    model_saved_path = args['train']['model_saved_path']\n",
    "    os.makedirs(model_saved_path, mode=0o755, exist_ok=True)\n",
    "    save_path = os.path.join(model_saved_path, 'model-{}.pt'.format(epoch))\n",
    "\n",
    "    num_updates = _train_one_epoch(model, train_loader, epoch, optimizer, lr_scheduler, num_updates, args)\n",
    "    \n",
    "    # model save\n",
    "    state_dict = {\n",
    "        'num_updates': num_updates,\n",
    "        'config': args,\n",
    "        'model_parameters': model.state_dict(),\n",
    "    }\n",
    "    torch.save(state_dict, save_path)\n",
    "    print('save model to {}, num_updates {}.'.format(save_path, num_updates))\n",
    "    \n",
    "    results = eval_cpl(model, test_loader)\n",
    "    if best_results is None or results['R@1,mIoU'].avg > best_results['R@1,mIoU'].avg:\n",
    "        best_results = results\n",
    "        os.system('cp %s %s'%(save_path, os.path.join(model_saved_path, 'model-best.pt')))\n",
    "        print('Best results have been updated.')\n",
    "    print('=' * 60)\n",
    "\n",
    "msg = '|'.join([' {} {:.4f} '.format(k, v.avg) for k, v in best_results.items()])\n",
    "print('Best results:')\n",
    "print('|'+msg+'|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/dataset/cpl/cpl/checkpoints/charades/default_charades/model-best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model load\n",
    "state_dict = torch.load(model_path)\n",
    "parameters = state_dict['model_parameters']\n",
    "model.load_state_dict(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| R@1,mIoU 0.4382 | R@1,IoU@0.1 0.7606 | R@1,IoU@0.3 0.6700 | R@1,IoU@0.5 0.4927 | R@1,IoU@0.7 0.2289 | R@1,IoU@0.9 0.0361 | R@5,mIoU 0.6701 | R@5,IoU@0.1 0.9883 | R@5,IoU@0.3 0.9674 | R@5,IoU@0.5 0.8319 | R@5,IoU@0.7 0.5032 | R@5,IoU@0.9 0.0836 |\n"
     ]
    }
   ],
   "source": [
    "results = eval_cpl(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
